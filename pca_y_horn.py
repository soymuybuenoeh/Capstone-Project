# -*- coding: utf-8 -*-
"""PCA Y HORN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R18z5PqWnvBJ1FiYm4qc7YnQK49M9lws

# Pre-processing
"""

pip install wikipedia

!pip install ace_tools

!pip install transformers

!pip install tqdm

!pip install datasets

# prompt: load the data_clean.csv file

import pandas as pd

# Load the dataframe.
try:
    df = pd.read_csv('data_clean.csv')
    print("Data loaded successfully.")
except FileNotFoundError:
    print("Error: 'data_clean.csv' not found. Please upload the file.")
except pd.errors.EmptyDataError:
    print("Error: 'data_clean.csv' is empty.")
except pd.errors.ParserError:
    print("Error: Unable to parse 'data_clean.csv'. Please check the file format.")

import torch
import numpy as np
from transformers import BertTokenizer, BertModel
import wikipedia

# Configurar Wikipedia para que use el idioma español
wikipedia.set_lang("en")

# Cargar el modelo y el tokenizador para español
#model_name = "dccuchile/bert-base-spanish-wwm-cased"
#model_name = "bert-base-multilingual-cased"
model_name = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

"""# Medidas Saliencia"""

# Seleccionar las columnas desde "ghana" hasta "oman"
columnas_interes = df.loc[:, "ghana":"oman"]

# Calcular la media de cada columna
media_columnas = columnas_interes.mean().sort_values(ascending=False)

# Mostrar el resultado
print(media_columnas)

df_long

import pandas as pd

# Suponiendo que df_long es el DataFrame que muestras en la captura,
# con las columnas: Relación, condition_order, País, Conteo.

# 1. Pivotar para convertir condition_order en columnas (AB y BA),
#    acumulando los valores de Conteo.
df_pivot = df_long.pivot_table(
    index=["Relación", "País"],      # Filas únicas definidas por (Relación, País)
    columns="condition_order",       # Columnas serán AB y BA
    values="Conteo",                 # Los valores a colocar en la intersección
    fill_value=0                     # Reemplaza NaN por 0
).reset_index()

# 2. Renombrar columnas para mayor claridad (opcional)
df_pivot.rename(columns={"AB": "AB", "BA": "BA"}, inplace=True)

# 3. Mostrar el resultado
print(df_pivot)

# Opcional: exportar a CSV
# df_pivot.to_csv("df_long_con_AB_y_BA.csv", index=False, encoding="utf-8")

# Filtrar las columnas que siguen el patrón solicitado
start_col = "ghana.luxemburgo_s"
end_col = "oman.yemen_s"
columns_of_interest = df.loc[:, start_col:end_col]

# Contar las ocurrencias de "a" y "d" en cada columna
counts = columns_of_interest.apply(lambda col: col.value_counts()).fillna(0)

# Convertir el índice en una columna para mejor visualización
counts = counts.T.reset_index()
counts.rename(columns={"index": "Relacion", "a": "Cantidad A", "d": "Cantidad D"}, inplace=True)

# Mostrar el DataFrame
display(counts)

# Calculate percentages for 'A' and 'D'
counts['Porcentaje A'] = (counts['Cantidad A'] / (counts['Cantidad A'] + counts['Cantidad D'])) * 100
counts['Porcentaje D'] = (counts['Cantidad D'] / (counts['Cantidad A'] + counts['Cantidad D'])) * 100

# Fill NaN values with 0 (if a category is not present in a comparison)
counts.fillna(0, inplace=True)

# Format percentage columns
pd.options.display.float_format = '{:.2f}%'.format

# Display the updated DataFrame
display(counts)

pip install pycountry rapidfuzz

def extraer_texto(row):
    # Se usa la columna "Relacion" renombrada previamente
    relacion = row["Relacion"]
    por_a = row["Porcentaje A"]
    por_d = row["Porcentaje D"]

    if por_a > por_d:
        # Si Porcentaje A es mayor, se extrae el texto entre el punto y el guion bajo
        partes = relacion.split('.')
        if len(partes) > 1:
            return partes[1].split('_')[0]
    elif por_d > por_a:
        # Si Porcentaje D es mayor, se extrae el texto antes del punto
        return relacion.split('.')[0]
    return ""  # En caso de empate o formato inesperado

# Aplicar la función a cada fila y almacenar el resultado en la nueva columna "Pais con mas saliencia"
counts["Pais"] = counts.apply(extraer_texto, axis=1)

# Mostrar el DataFrame final
display(counts)

"""# My method"""

from datasets import load_dataset
from transformers import BertTokenizer, BertModel
import torch
import numpy as np

# 1. Cargar Wikipedia
dataset = load_dataset("wikipedia", "20220301.en", split="train")

# 2. Filtrar artículos que contienen "China"
china_articles = [entry["text"] for entry in dataset if "China" in entry["text"]][:30]  # Limita a 30 artículos

# 3. Cargar modelo BERT base
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

# 4. Obtener embeddings del término "China"
def extract_embeddings(text, keyword="china"):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
    indices = [i for i, token in enumerate(tokens) if keyword.lower() in token.lower()]
    if indices:
        return outputs.last_hidden_state[0, indices, :].numpy()
    return []

# 5. Construir el contorno semántico (lista de embeddings de "China")
contorno_china = []
for article in china_articles:
    contorno_china.extend(extract_embeddings(article, "china"))

contorno_china = np.array(contorno_china)
print(f"Se han extraído {contorno_china.shape[0]} embeddings del término 'China'.")

from sklearn.decomposition import PCA
from sklearn.utils import resample

# 1. Aplicar PCA al contorno real
pca_real = PCA()
pca_real.fit(contorno_china)
explained_real = pca_real.explained_variance_

# 2. Generar datos aleatorios simulados con la misma forma y distribución
num_iterations = 100
simulated_variances = []

for _ in range(num_iterations):
    shuffled = np.random.normal(size=contorno_china.shape)
    pca_shuffled = PCA()
    pca_shuffled.fit(shuffled)
    simulated_variances.append(pca_shuffled.explained_variance_)

# 3. Calcular la media de las varianzas simuladas
mean_simulated = np.mean(simulated_variances, axis=0)

# 4. Comparar: ¿cuántas componentes reales superan las simuladas?
num_dim_optimo = sum(explained_real > mean_simulated)

print(f"Número óptimo de dimensiones según el Análisis Paralelo de Horn: {num_dim_optimo}")

import matplotlib.pyplot as plt

plt.plot(explained_real[:50], label="Varianza real (PCA)")
plt.plot(mean_simulated[:50], label="Varianza esperada (simulada)")
plt.axvline(num_dim_optimo, color="red", linestyle="--", label=f"{num_dim_optimo} dimensiones óptimas")
plt.xlabel("Componentes principales")
plt.ylabel("Varianza explicada")
plt.title("Análisis Paralelo de Horn sobre el contorno de 'China'")
plt.legend()
plt.show()

import pandas as pd
import numpy as np

# Simulación de embeddings de ejemplo
np.random.seed(42)
embeddings = np.random.normal(size=(841, 768))

# Crear DataFrame con nombres de columnas
columnas = [f"Dim. {i+1}" for i in range(768)]
df = pd.DataFrame(embeddings, columns=columnas)

# Insertar columna con etiquetas tipo "China 1", ..., "China 841"
df.insert(0, "Términos", [f"China {i+1}" for i in range(841)])

# Mostrar una tabla estilo resumen como en la imagen
df_muestra = pd.concat([df.head(3), pd.DataFrame([["…"] + ["…"]*768], columns=df.columns), df.tail(1)])
print(df_muestra.iloc[:, :5].to_string(index=False))  # Solo muestra hasta Dim. 4 para imitar la tabla recortada

# Exportar a LaTeX (recortada como ejemplo)
latex_table = df_muestra.iloc[:, list(range(5)) + [-1]].to_latex(index=False, column_format="lrrrrrl")
print(latex_table)

"""# My method 2"""

import wikipedia
from transformers import BertTokenizer, BertModel
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.utils import check_random_state
from tqdm import tqdm

# ---------------------------
# CONFIGURACIÓN
# ---------------------------
wikipedia.set_lang("en")

countries = [
    "ghana", "belize", "slovenia", "slovakia", "zambia", "zimbabwe", "laos", "cambodia", "yemen",
    "turkmenistan", "uzbekistan", "luxembourg", "france", "bolivia", "united states", "china", "mexico", "russia",
    "belgium", "andorra", "spain", "belarus", "faroe islands", "argentina", "paraguay", "brazil",
    "germany", "canada", "japan", "italy", "morocco", "india", "united kingdom", "singapore", "sweden", "norway",
    "portugal", "ecuador", "peru", "burkina faso", "french guiana", "mongolia", "bulgaria", "oman"
]

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

# ---------------------------
# FUNCIONES
# ---------------------------

def fetch_paragraphs_with_query(query, max_results=30):
    titles = wikipedia.search(query, results=max_results)
    relevant_paragraphs = []
    for title in titles:
        try:
            content = wikipedia.page(title).content
            paragraphs = content.split('\n\n')
            for paragraph in paragraphs:
                if query.lower() in paragraph.lower():
                    relevant_paragraphs.append(paragraph)
        except (wikipedia.exceptions.DisambiguationError, wikipedia.exceptions.PageError):
            continue
    return relevant_paragraphs

def get_bert_embeddings(paragraphs, word):
    all_embeddings = []
    for paragraph in paragraphs:
        inputs = tokenizer(paragraph, return_tensors="pt", truncation=True, padding="max_length", max_length=512)
        with torch.no_grad():
            outputs = model(**inputs)
        last_hidden_state = outputs.last_hidden_state
        word_ids = tokenizer.encode(word, add_special_tokens=False)
        word_idx = (inputs["input_ids"] == word_ids[0]).nonzero(as_tuple=False)
        paragraph_embeddings = [last_hidden_state[0][idx[1]].numpy() for idx in word_idx]
        all_embeddings.extend(paragraph_embeddings)
    return all_embeddings

def horn_parallel_analysis(data, n_permutations=100, random_state=None, label="", plot=True):
    random_state = check_random_state(random_state)
    n_vars, n_samples = data.shape
    real_pca = PCA().fit(data)
    real_eigenvalues = real_pca.explained_variance_

    rand_eigenvalues = []
    for _ in tqdm(range(n_permutations), desc=f"Horn Analysis: {label}"):
        random_data = random_state.normal(size=data.shape)
        random_pca = PCA().fit(random_data)
        rand_eigenvalues.append(random_pca.explained_variance_)

    eigenvalue_thresholds = np.percentile(rand_eigenvalues, 95, axis=0)
    n_components_to_retain = np.sum(real_eigenvalues > eigenvalue_thresholds)

    if plot:
        plt.figure(figsize=(8, 4))
        plt.plot(real_eigenvalues[:50], label="Real Eigenvalues", color="blue")
        plt.plot(eigenvalue_thresholds[:50], label="95th percentile Random Eigenvalues", color="red")
        plt.axvline(n_components_to_retain, color="green", linestyle="--", label=f"{n_components_to_retain} dims")
        plt.title(f"Horn Parallel Analysis - {label.title()}")
        plt.xlabel("Components")
        plt.ylabel("Eigenvalue")
        plt.legend()
        plt.tight_layout()
        plt.show()

    return n_components_to_retain

# ---------------------------
# EJECUCIÓN POR PAÍS
# ---------------------------

for country in countries:
    print(f"\n🌍 Procesando país: {country.title()}")
    paragraphs = fetch_paragraphs_with_query(country, max_results=30)

    if not paragraphs:
        print(f"❌ No se encontraron párrafos.")
        continue

    embeddings = get_bert_embeddings(paragraphs, country.lower())
    if len(embeddings) < 10:
        print(f"⚠️ Solo {len(embeddings)} embeddings encontrados. Saltando análisis.")
        continue

    X = np.array(embeddings).T
    print(f"✔️ Matriz embeddings: {X.shape[1]} tokens × {X.shape[0]} dimensiones")

    try:
        n_components = horn_parallel_analysis(X, random_state=42, label=country)
        print(f"📐 Dimensiones óptimas para {country.title()}: {n_components}")

        # PCA final para subespacio
        pca = PCA(n_components=n_components)
        subspace = pca.fit_transform(X.T).T

        print(f"📏 Subespacio reducido: {subspace.shape}")

    except Exception as e:
        print(f"❌ Error procesando {country.title()}: {e}")

"""# PCA y Horn method A"""

import wikipedia

def fetch_paragraphs_with_query(query, max_results=100):
    # Buscar páginas que coincidan con la consulta
    titles = wikipedia.search(query, results=max_results)

    relevant_paragraphs = []
    for title in titles:
        try:
            content = wikipedia.page(title).content
            paragraphs = content.split('\n\n')  # Dividir el contenido en párrafos

            # Filtrar párrafos que contienen la palabra "bat"
            for paragraph in paragraphs:
                if query.lower() in paragraph.lower():
                    relevant_paragraphs.append(paragraph)

        except (wikipedia.exceptions.DisambiguationError, wikipedia.exceptions.PageError):
            continue

    return relevant_paragraphs

china_paragraphs = fetch_paragraphs_with_query("China", max_results=30)
len(china_paragraphs)

from transformers import BertTokenizer, BertModel
import torch

def get_bert_embeddings(paragraphs, word):
    # Cargar el tokenizador y el modelo de BERT
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')

    all_embeddings = []

    # Procesar cada párrafo
    for paragraph in paragraphs:
        # Tokenizar y obtener salida de BERT
        inputs = tokenizer(paragraph, return_tensors="pt", truncation=True, max_length=512, padding='max_length')
        with torch.no_grad():
            outputs = model(**inputs)

        # Tomar embeddings del último layer (cuarentena!!!!)
        last_hidden_state = outputs.last_hidden_state

        # Buscar índices de la palabra "bat"
        word_ids = tokenizer.encode(word, add_special_tokens=False)
        word_idx = (inputs["input_ids"] == word_ids[0]).nonzero(as_tuple=False)

        # Si la palabra "bat" está en el párrafo, recoger todos los embeddings para cada aparición
        paragraph_embeddings = [last_hidden_state[0][idx[1]].numpy() for idx in word_idx]
        all_embeddings.extend(paragraph_embeddings)

    return all_embeddings

# Obtener embeddings para la palabra "bat" en todos los párrafos relevantes
china_embeddings = get_bert_embeddings(china_paragraphs, "China")
len(china_embeddings)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.utils import check_random_state
from tqdm import tqdm

def horn_parallel_analysis(data, n_permutations=100, random_state=None):
    random_state = check_random_state(random_state)
    n_vars, n_samples = data.shape
    real_pca = PCA().fit(data)
    real_eigenvalues = real_pca.explained_variance_

    rand_eigenvalues = []

    # Using tqdm for progress bar
    for _ in tqdm(range(n_permutations), desc="Running Parallel Analysis"):
        random_data = random_state.normal(size=data.shape)
        random_pca = PCA().fit(random_data)
        rand_eigenvalues.append(random_pca.explained_variance_)

    eigenvalue_thresholds = np.percentile(rand_eigenvalues, 95, axis=0)

    n_components_to_retain = np.sum(real_eigenvalues > eigenvalue_thresholds)

    # Plotting
    plt.plot(real_eigenvalues, label="Real Eigenvalues", color="blue")
    plt.plot(eigenvalue_thresholds, label="95th percentile Random Eigenvalues", color="red")
    plt.xlabel("Components")
    plt.ylabel("Eigenvalue")
    plt.legend()
    plt.title(f"Parallel Analysis: {n_components_to_retain} components to retain")
    plt.show()

    return n_components_to_retain

# Generate some example data
X = np.array(china_embeddings).T

# Use function
n_components = horn_parallel_analysis(X)
print(f"Number of components to retain: {n_components}")

"""# Method B

## Intento 1
"""

import wikipedia
from transformers import BertTokenizer, BertModel
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.utils import check_random_state
from tqdm import tqdm

# Configurar Wikipedia en inglés
wikipedia.set_lang("en")

# Lista de países
countries = [
    "ghana", "belize", "slovenia", "slovakia", "zambia", "zimbabwe", "laos", "cambodia", "yemen",
    "turkmenistan", "uzbekistan", "luxembourg", "france", "bolivia", "united states", "china", "mexico", "russia",
    "belgium", "andorra", "spain", "belarus", "faroe islands", "argentina", "paraguay", "brazil",
    "germany", "canada", "japan", "italy", "morocco", "india", "united kingdom", "singapore", "sweden", "norway",
    "portugal", "ecuador", "peru", "burkina faso", "french guiana", "mongolia", "bulgaria", "oman"
]

# Función para obtener párrafos relevantes desde Wikipedia
def fetch_paragraphs_with_query(query, max_results=30):
    titles = wikipedia.search(query, results=max_results)
    relevant_paragraphs = []
    for title in titles:
        try:
            content = wikipedia.page(title).content
            paragraphs = content.split('\n\n')
            for paragraph in paragraphs:
                if query.lower() in paragraph.lower():
                    relevant_paragraphs.append(paragraph)
        except (wikipedia.exceptions.DisambiguationError, wikipedia.exceptions.PageError):
            continue
    return relevant_paragraphs

# Función para obtener embeddings de BERT de un término en contexto
def get_bert_embeddings(paragraphs, word):
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')
    all_embeddings = []

    for paragraph in paragraphs:
        inputs = tokenizer(paragraph, return_tensors="pt", truncation=True, max_length=512, padding='max_length')
        with torch.no_grad():
            outputs = model(**inputs)

        last_hidden_state = outputs.last_hidden_state
        word_ids = tokenizer.encode(word, add_special_tokens=False)
        word_idx = (inputs["input_ids"] == word_ids[0]).nonzero(as_tuple=False)
        paragraph_embeddings = [last_hidden_state[0][idx[1]].numpy() for idx in word_idx]
        all_embeddings.extend(paragraph_embeddings)

    return all_embeddings

# Análisis Paralelo de Horn
def horn_parallel_analysis(data, n_permutations=100, random_state=None, plot=True, label=""):
    random_state = check_random_state(random_state)
    n_vars, n_samples = data.shape
    real_pca = PCA().fit(data)
    real_eigenvalues = real_pca.explained_variance_

    rand_eigenvalues = []
    for _ in tqdm(range(n_permutations), desc="Running Parallel Analysis"):
        random_data = random_state.normal(size=data.shape)
        random_pca = PCA().fit(random_data)
        rand_eigenvalues.append(random_pca.explained_variance_)

    eigenvalue_thresholds = np.percentile(rand_eigenvalues, 95, axis=0)
    n_components_to_retain = np.sum(real_eigenvalues > eigenvalue_thresholds)

    if plot:
        plt.figure(figsize=(8, 4))
        plt.plot(real_eigenvalues, label="Real Eigenvalues", color="blue")
        plt.plot(eigenvalue_thresholds, label="95th percentile Random Eigenvalues", color="red")
        plt.axvline(n_components_to_retain, color="green", linestyle="--", label=f"{n_components_to_retain} dims")
        plt.xlabel("Components")
        plt.ylabel("Eigenvalue")
        plt.title(f"Horn Parallel Analysis - {label}")
        plt.legend()
        plt.tight_layout()
        plt.show()

    return n_components_to_retain

# Loop sobre países
for country in countries:
    print(f"\n🌍 Procesando país: {country.title()}")
    paragraphs = fetch_paragraphs_with_query(country, max_results=30)
    if not paragraphs:
        print(f"❌ No se encontraron párrafos para {country.title()}.")
        continue

    embeddings = get_bert_embeddings(paragraphs, country.lower())
    if len(embeddings) < 10:
        print(f"⚠️ Muy pocos embeddings ({len(embeddings)}). Saltando análisis.")
        continue

    X = np.array(embeddings).T
    print(f"✔️ Total embeddings: {X.shape[1]} (cada uno con {X.shape[0]} dimensiones)")

    # Análisis paralelo
    try:
        n_components = horn_parallel_analysis(X, n_permutations=100, random_state=42, plot=True, label=country.title())
        print(f"📐 Dimensiones óptimas para {country.title()}: {n_components}")

        # PCA final para obtener subespacio reducido
        pca = PCA(n_components=n_components)
        subspace = pca.fit_transform(X).T  # Transpuesta para conservar forma original

        print(f"📏 Subespacio reducido: {subspace.shape}")
    except Exception as e:
        print(f"❌ Error procesando {country.title()}: {e}")

"""## Intento 2"""

import wikipedia
from transformers import BertTokenizer, BertModel
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.utils import check_random_state
from tqdm import tqdm

# Configurar Wikipedia en inglés
wikipedia.set_lang("en")

# Lista de países
countries = [ "china", "mexico", "russia",
    "belgium", "andorra", "spain", "belarus", "faroe islands", "argentina", "paraguay", "brazil",
    "germany", "canada", "japan", "italy", "morocco", "india", "united kingdom", "singapore", "sweden", "norway",
    "portugal", "ecuador", "peru", "burkina faso", "french guiana", "mongolia", "bulgaria", "oman"
]

# Función para obtener párrafos relevantes desde Wikipedia
def fetch_paragraphs_with_query(query, max_results=30):
    titles = wikipedia.search(query, results=max_results)
    relevant_paragraphs = []
    for title in titles:
        try:
            content = wikipedia.page(title).content
            paragraphs = content.split('\n\n')
            for paragraph in paragraphs:
                if query.lower() in paragraph.lower():
                    relevant_paragraphs.append(paragraph)
        except (wikipedia.exceptions.DisambiguationError, wikipedia.exceptions.PageError):
            continue
    return relevant_paragraphs

# Función para obtener embeddings de BERT de un término en contexto
def get_bert_embeddings(paragraphs, word):
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')
    all_embeddings = []

    for paragraph in paragraphs:
        inputs = tokenizer(paragraph, return_tensors="pt", truncation=True, max_length=512, padding='max_length')
        with torch.no_grad():
            outputs = model(**inputs)

        last_hidden_state = outputs.last_hidden_state
        word_ids = tokenizer.encode(word, add_special_tokens=False)
        word_idx = (inputs["input_ids"] == word_ids[0]).nonzero(as_tuple=False)
        paragraph_embeddings = [last_hidden_state[0][idx[1]].numpy() for idx in word_idx]
        all_embeddings.extend(paragraph_embeddings)

    return all_embeddings

# Análisis Paralelo de Horn
def horn_parallel_analysis(data, n_permutations=100, random_state=None, plot=True, label=""):
    random_state = check_random_state(random_state)
    n_vars, n_samples = data.shape
    real_pca = PCA().fit(data)
    real_eigenvalues = real_pca.explained_variance_

    rand_eigenvalues = []
    for _ in tqdm(range(n_permutations), desc="Running Parallel Analysis"):
        random_data = random_state.normal(size=data.shape)
        random_pca = PCA().fit(random_data)
        rand_eigenvalues.append(random_pca.explained_variance_)

    eigenvalue_thresholds = np.percentile(rand_eigenvalues, 95, axis=0)
    n_components_to_retain = np.sum(real_eigenvalues > eigenvalue_thresholds)

    if plot:
        plt.figure(figsize=(8, 4))
        plt.plot(real_eigenvalues, label="Real Eigenvalues", color="blue")
        plt.plot(eigenvalue_thresholds, label="95th percentile Random Eigenvalues", color="red")
        plt.axvline(n_components_to_retain, color="green", linestyle="--", label=f"{n_components_to_retain} dims")
        plt.xlabel("Components")
        plt.ylabel("Eigenvalue")
        plt.title(f"Horn Parallel Analysis - {label}")
        plt.legend()
        plt.tight_layout()
        plt.show()

    return n_components_to_retain

# Loop sobre países
for country in countries:
    print(f"\n🌍 Procesando país: {country.title()}")
    paragraphs = fetch_paragraphs_with_query(country, max_results=30)
    if not paragraphs:
        print(f"❌ No se encontraron párrafos para {country.title()}.")
        continue

    embeddings = get_bert_embeddings(paragraphs, country.lower())
    if len(embeddings) < 10:
        print(f"⚠️ Muy pocos embeddings ({len(embeddings)}). Saltando análisis.")
        continue

    X = np.array(embeddings).T
    print(f"✔️ Total embeddings: {X.shape[1]} (cada uno con {X.shape[0]} dimensiones)")

    # Análisis paralelo
    try:
        n_components = horn_parallel_analysis(X, n_permutations=100, random_state=42, plot=True, label=country.title())
        print(f"📐 Dimensiones óptimas para {country.title()}: {n_components}")

        # PCA final para obtener subespacio reducido
        pca = PCA(n_components=n_components)
        subspace = pca.fit_transform(X.T).T  # Transpuesta para conservar forma original

        print(f"📏 Subespacio reducido: {subspace.shape}")
    except Exception as e:
        print(f"❌ Error procesando {country.title()}: {e}")

"""## Intento 3"""

import wikipedia
from transformers import BertTokenizer, BertModel
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.utils import check_random_state
from tqdm import tqdm

# Configurar Wikipedia en inglés
wikipedia.set_lang("en")

# Lista de países
countries = [ "argentina", "paraguay", "brazil",
    "germany", "canada", "japan", "italy", "morocco", "india", "united kingdom", "singapore", "sweden", "norway",
    "portugal", "ecuador", "peru", "burkina faso", "french guiana", "mongolia", "bulgaria", "oman"
]

# Función para obtener párrafos relevantes desde Wikipedia
def fetch_paragraphs_with_query(query, max_results=30):
    titles = wikipedia.search(query, results=max_results)
    relevant_paragraphs = []
    for title in titles:
        try:
            content = wikipedia.page(title).content
            paragraphs = content.split('\n\n')
            for paragraph in paragraphs:
                if query.lower() in paragraph.lower():
                    relevant_paragraphs.append(paragraph)
        except (wikipedia.exceptions.DisambiguationError, wikipedia.exceptions.PageError):
            continue
    return relevant_paragraphs

# Función para obtener embeddings de BERT de un término en contexto
def get_bert_embeddings(paragraphs, word):
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')
    all_embeddings = []

    for paragraph in paragraphs:
        inputs = tokenizer(paragraph, return_tensors="pt", truncation=True, max_length=512, padding='max_length')
        with torch.no_grad():
            outputs = model(**inputs)

        last_hidden_state = outputs.last_hidden_state
        word_ids = tokenizer.encode(word, add_special_tokens=False)
        word_idx = (inputs["input_ids"] == word_ids[0]).nonzero(as_tuple=False)
        paragraph_embeddings = [last_hidden_state[0][idx[1]].numpy() for idx in word_idx]
        all_embeddings.extend(paragraph_embeddings)

    return all_embeddings

# Análisis Paralelo de Horn
def horn_parallel_analysis(data, n_permutations=100, random_state=None, plot=True, label=""):
    random_state = check_random_state(random_state)
    n_vars, n_samples = data.shape
    real_pca = PCA().fit(data)
    real_eigenvalues = real_pca.explained_variance_

    rand_eigenvalues = []
    for _ in tqdm(range(n_permutations), desc="Running Parallel Analysis"):
        random_data = random_state.normal(size=data.shape)
        random_pca = PCA().fit(random_data)
        rand_eigenvalues.append(random_pca.explained_variance_)

    eigenvalue_thresholds = np.percentile(rand_eigenvalues, 95, axis=0)
    n_components_to_retain = np.sum(real_eigenvalues > eigenvalue_thresholds)

    if plot:
        plt.figure(figsize=(8, 4))
        plt.plot(real_eigenvalues, label="Real Eigenvalues", color="blue")
        plt.plot(eigenvalue_thresholds, label="95th percentile Random Eigenvalues", color="red")
        plt.axvline(n_components_to_retain, color="green", linestyle="--", label=f"{n_components_to_retain} dims")
        plt.xlabel("Components")
        plt.ylabel("Eigenvalue")
        plt.title(f"Horn Parallel Analysis - {label}")
        plt.legend()
        plt.tight_layout()
        plt.show()

    return n_components_to_retain

# Loop sobre países
for country in countries:
    print(f"\n🌍 Procesando país: {country.title()}")
    paragraphs = fetch_paragraphs_with_query(country, max_results=30)
    if not paragraphs:
        print(f"❌ No se encontraron párrafos para {country.title()}.")
        continue

    embeddings = get_bert_embeddings(paragraphs, country.lower())
    if len(embeddings) < 10:
        print(f"⚠️ Muy pocos embeddings ({len(embeddings)}). Saltando análisis.")
        continue

    X = np.array(embeddings).T
    print(f"✔️ Total embeddings: {X.shape[1]} (cada uno con {X.shape[0]} dimensiones)")

    # Análisis paralelo
    try:
        n_components = horn_parallel_analysis(X, n_permutations=100, random_state=42, plot=True, label=country.title())
        print(f"📐 Dimensiones óptimas para {country.title()}: {n_components}")

        # PCA final para obtener subespacio reducido
        pca = PCA(n_components=n_components)
        subspace = pca.fit_transform(X.T).T  # Transpuesta para conservar forma original

        print(f"📏 Subespacio reducido: {subspace.shape}")
    except Exception as e:
        print(f"❌ Error procesando {country.title()}: {e}")

"""## Intento 4"""

import wikipedia
from transformers import BertTokenizer, BertModel
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.utils import check_random_state
from tqdm import tqdm

# Configurar Wikipedia en inglés
wikipedia.set_lang("en")

# Lista de países
countries = [  "morocco", "india", "united kingdom", "singapore", "sweden", "norway",
    "portugal", "ecuador", "peru", "burkina faso", "french guiana", "mongolia", "bulgaria", "oman"
]

# Función para obtener párrafos relevantes desde Wikipedia
def fetch_paragraphs_with_query(query, max_results=30):
    titles = wikipedia.search(query, results=max_results)
    relevant_paragraphs = []
    for title in titles:
        try:
            content = wikipedia.page(title).content
            paragraphs = content.split('\n\n')
            for paragraph in paragraphs:
                if query.lower() in paragraph.lower():
                    relevant_paragraphs.append(paragraph)
        except (wikipedia.exceptions.DisambiguationError, wikipedia.exceptions.PageError):
            continue
    return relevant_paragraphs

# Función para obtener embeddings de BERT de un término en contexto
def get_bert_embeddings(paragraphs, word):
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')
    all_embeddings = []

    for paragraph in paragraphs:
        inputs = tokenizer(paragraph, return_tensors="pt", truncation=True, max_length=512, padding='max_length')
        with torch.no_grad():
            outputs = model(**inputs)

        last_hidden_state = outputs.last_hidden_state
        word_ids = tokenizer.encode(word, add_special_tokens=False)
        word_idx = (inputs["input_ids"] == word_ids[0]).nonzero(as_tuple=False)
        paragraph_embeddings = [last_hidden_state[0][idx[1]].numpy() for idx in word_idx]
        all_embeddings.extend(paragraph_embeddings)

    return all_embeddings

# Análisis Paralelo de Horn
def horn_parallel_analysis(data, n_permutations=100, random_state=None, plot=True, label=""):
    random_state = check_random_state(random_state)
    n_vars, n_samples = data.shape
    real_pca = PCA().fit(data)
    real_eigenvalues = real_pca.explained_variance_

    rand_eigenvalues = []
    for _ in tqdm(range(n_permutations), desc="Running Parallel Analysis"):
        random_data = random_state.normal(size=data.shape)
        random_pca = PCA().fit(random_data)
        rand_eigenvalues.append(random_pca.explained_variance_)

    eigenvalue_thresholds = np.percentile(rand_eigenvalues, 95, axis=0)
    n_components_to_retain = np.sum(real_eigenvalues > eigenvalue_thresholds)

    if plot:
        plt.figure(figsize=(8, 4))
        plt.plot(real_eigenvalues, label="Real Eigenvalues", color="blue")
        plt.plot(eigenvalue_thresholds, label="95th percentile Random Eigenvalues", color="red")
        plt.axvline(n_components_to_retain, color="green", linestyle="--", label=f"{n_components_to_retain} dims")
        plt.xlabel("Components")
        plt.ylabel("Eigenvalue")
        plt.title(f"Horn Parallel Analysis - {label}")
        plt.legend()
        plt.tight_layout()
        plt.show()

    return n_components_to_retain

# Loop sobre países
for country in countries:
    print(f"\n🌍 Procesando país: {country.title()}")
    paragraphs = fetch_paragraphs_with_query(country, max_results=30)
    if not paragraphs:
        print(f"❌ No se encontraron párrafos para {country.title()}.")
        continue

    embeddings = get_bert_embeddings(paragraphs, country.lower())
    if len(embeddings) < 10:
        print(f"⚠️ Muy pocos embeddings ({len(embeddings)}). Saltando análisis.")
        continue

    X = np.array(embeddings).T
    print(f"✔️ Total embeddings: {X.shape[1]} (cada uno con {X.shape[0]} dimensiones)")

    # Análisis paralelo
    try:
        n_components = horn_parallel_analysis(X, n_permutations=100, random_state=42, plot=True, label=country.title())
        print(f"📐 Dimensiones óptimas para {country.title()}: {n_components}")

        # PCA final para obtener subespacio reducido
        pca = PCA(n_components=n_components)
        subspace = pca.fit_transform(X.T).T  # Transpuesta para conservar forma original

        print(f"📏 Subespacio reducido: {subspace.shape}")
    except Exception as e:
        print(f"❌ Error procesando {country.title()}: {e}")