# -*- coding: utf-8 -*-
"""PCA Y HORN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R18z5PqWnvBJ1FiYm4qc7YnQK49M9lws

# Pre-processing
"""

pip install wikipedia

!pip install ace_tools

!pip install transformers

!pip install tqdm

!pip install datasets

# prompt: load the data_clean.csv file

import pandas as pd

# Load the dataframe.
try:
    df = pd.read_csv('data_clean.csv')
    print("Data loaded successfully.")
except FileNotFoundError:
    print("Error: 'data_clean.csv' not found. Please upload the file.")
except pd.errors.EmptyDataError:
    print("Error: 'data_clean.csv' is empty.")
except pd.errors.ParserError:
    print("Error: Unable to parse 'data_clean.csv'. Please check the file format.")

import torch
import numpy as np
from transformers import BertTokenizer, BertModel
import wikipedia

# Configurar Wikipedia para que use el idioma espa√±ol
wikipedia.set_lang("en")

# Cargar el modelo y el tokenizador para espa√±ol
#model_name = "dccuchile/bert-base-spanish-wwm-cased"
#model_name = "bert-base-multilingual-cased"
model_name = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

"""# Medidas Saliencia"""

# Seleccionar las columnas desde "ghana" hasta "oman"
columnas_interes = df.loc[:, "ghana":"oman"]

# Calcular la media de cada columna
media_columnas = columnas_interes.mean().sort_values(ascending=False)

# Mostrar el resultado
print(media_columnas)

df_long

import pandas as pd

# Suponiendo que df_long es el DataFrame que muestras en la captura,
# con las columnas: Relaci√≥n, condition_order, Pa√≠s, Conteo.

# 1. Pivotar para convertir condition_order en columnas (AB y BA),
#    acumulando los valores de Conteo.
df_pivot = df_long.pivot_table(
    index=["Relaci√≥n", "Pa√≠s"],      # Filas √∫nicas definidas por (Relaci√≥n, Pa√≠s)
    columns="condition_order",       # Columnas ser√°n AB y BA
    values="Conteo",                 # Los valores a colocar en la intersecci√≥n
    fill_value=0                     # Reemplaza NaN por 0
).reset_index()

# 2. Renombrar columnas para mayor claridad (opcional)
df_pivot.rename(columns={"AB": "AB", "BA": "BA"}, inplace=True)

# 3. Mostrar el resultado
print(df_pivot)

# Opcional: exportar a CSV
# df_pivot.to_csv("df_long_con_AB_y_BA.csv", index=False, encoding="utf-8")

# Filtrar las columnas que siguen el patr√≥n solicitado
start_col = "ghana.luxemburgo_s"
end_col = "oman.yemen_s"
columns_of_interest = df.loc[:, start_col:end_col]

# Contar las ocurrencias de "a" y "d" en cada columna
counts = columns_of_interest.apply(lambda col: col.value_counts()).fillna(0)

# Convertir el √≠ndice en una columna para mejor visualizaci√≥n
counts = counts.T.reset_index()
counts.rename(columns={"index": "Relacion", "a": "Cantidad A", "d": "Cantidad D"}, inplace=True)

# Mostrar el DataFrame
display(counts)

# Calculate percentages for 'A' and 'D'
counts['Porcentaje A'] = (counts['Cantidad A'] / (counts['Cantidad A'] + counts['Cantidad D'])) * 100
counts['Porcentaje D'] = (counts['Cantidad D'] / (counts['Cantidad A'] + counts['Cantidad D'])) * 100

# Fill NaN values with 0 (if a category is not present in a comparison)
counts.fillna(0, inplace=True)

# Format percentage columns
pd.options.display.float_format = '{:.2f}%'.format

# Display the updated DataFrame
display(counts)

pip install pycountry rapidfuzz

def extraer_texto(row):
    # Se usa la columna "Relacion" renombrada previamente
    relacion = row["Relacion"]
    por_a = row["Porcentaje A"]
    por_d = row["Porcentaje D"]

    if por_a > por_d:
        # Si Porcentaje A es mayor, se extrae el texto entre el punto y el guion bajo
        partes = relacion.split('.')
        if len(partes) > 1:
            return partes[1].split('_')[0]
    elif por_d > por_a:
        # Si Porcentaje D es mayor, se extrae el texto antes del punto
        return relacion.split('.')[0]
    return ""  # En caso de empate o formato inesperado

# Aplicar la funci√≥n a cada fila y almacenar el resultado en la nueva columna "Pais con mas saliencia"
counts["Pais"] = counts.apply(extraer_texto, axis=1)

# Mostrar el DataFrame final
display(counts)

"""# My method"""

from datasets import load_dataset
from transformers import BertTokenizer, BertModel
import torch
import numpy as np

# 1. Cargar Wikipedia
dataset = load_dataset("wikipedia", "20220301.en", split="train")

# 2. Filtrar art√≠culos que contienen "China"
china_articles = [entry["text"] for entry in dataset if "China" in entry["text"]][:30]  # Limita a 30 art√≠culos

# 3. Cargar modelo BERT base
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

# 4. Obtener embeddings del t√©rmino "China"
def extract_embeddings(text, keyword="china"):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
    indices = [i for i, token in enumerate(tokens) if keyword.lower() in token.lower()]
    if indices:
        return outputs.last_hidden_state[0, indices, :].numpy()
    return []

# 5. Construir el contorno sem√°ntico (lista de embeddings de "China")
contorno_china = []
for article in china_articles:
    contorno_china.extend(extract_embeddings(article, "china"))

contorno_china = np.array(contorno_china)
print(f"Se han extra√≠do {contorno_china.shape[0]} embeddings del t√©rmino 'China'.")

from sklearn.decomposition import PCA
from sklearn.utils import resample

# 1. Aplicar PCA al contorno real
pca_real = PCA()
pca_real.fit(contorno_china)
explained_real = pca_real.explained_variance_

# 2. Generar datos aleatorios simulados con la misma forma y distribuci√≥n
num_iterations = 100
simulated_variances = []

for _ in range(num_iterations):
    shuffled = np.random.normal(size=contorno_china.shape)
    pca_shuffled = PCA()
    pca_shuffled.fit(shuffled)
    simulated_variances.append(pca_shuffled.explained_variance_)

# 3. Calcular la media de las varianzas simuladas
mean_simulated = np.mean(simulated_variances, axis=0)

# 4. Comparar: ¬øcu√°ntas componentes reales superan las simuladas?
num_dim_optimo = sum(explained_real > mean_simulated)

print(f"N√∫mero √≥ptimo de dimensiones seg√∫n el An√°lisis Paralelo de Horn: {num_dim_optimo}")

import matplotlib.pyplot as plt

plt.plot(explained_real[:50], label="Varianza real (PCA)")
plt.plot(mean_simulated[:50], label="Varianza esperada (simulada)")
plt.axvline(num_dim_optimo, color="red", linestyle="--", label=f"{num_dim_optimo} dimensiones √≥ptimas")
plt.xlabel("Componentes principales")
plt.ylabel("Varianza explicada")
plt.title("An√°lisis Paralelo de Horn sobre el contorno de 'China'")
plt.legend()
plt.show()

import pandas as pd
import numpy as np

# Simulaci√≥n de embeddings de ejemplo
np.random.seed(42)
embeddings = np.random.normal(size=(841, 768))

# Crear DataFrame con nombres de columnas
columnas = [f"Dim. {i+1}" for i in range(768)]
df = pd.DataFrame(embeddings, columns=columnas)

# Insertar columna con etiquetas tipo "China 1", ..., "China 841"
df.insert(0, "T√©rminos", [f"China {i+1}" for i in range(841)])

# Mostrar una tabla estilo resumen como en la imagen
df_muestra = pd.concat([df.head(3), pd.DataFrame([["‚Ä¶"] + ["‚Ä¶"]*768], columns=df.columns), df.tail(1)])
print(df_muestra.iloc[:, :5].to_string(index=False))  # Solo muestra hasta Dim. 4 para imitar la tabla recortada

# Exportar a LaTeX (recortada como ejemplo)
latex_table = df_muestra.iloc[:, list(range(5)) + [-1]].to_latex(index=False, column_format="lrrrrrl")
print(latex_table)

"""# My method 2"""

import wikipedia
from transformers import BertTokenizer, BertModel
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.utils import check_random_state
from tqdm import tqdm

# ---------------------------
# CONFIGURACI√ìN
# ---------------------------
wikipedia.set_lang("en")

countries = [
    "ghana", "belize", "slovenia", "slovakia", "zambia", "zimbabwe", "laos", "cambodia", "yemen",
    "turkmenistan", "uzbekistan", "luxembourg", "france", "bolivia", "united states", "china", "mexico", "russia",
    "belgium", "andorra", "spain", "belarus", "faroe islands", "argentina", "paraguay", "brazil",
    "germany", "canada", "japan", "italy", "morocco", "india", "united kingdom", "singapore", "sweden", "norway",
    "portugal", "ecuador", "peru", "burkina faso", "french guiana", "mongolia", "bulgaria", "oman"
]

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

# ---------------------------
# FUNCIONES
# ---------------------------

def fetch_paragraphs_with_query(query, max_results=30):
    titles = wikipedia.search(query, results=max_results)
    relevant_paragraphs = []
    for title in titles:
        try:
            content = wikipedia.page(title).content
            paragraphs = content.split('\n\n')
            for paragraph in paragraphs:
                if query.lower() in paragraph.lower():
                    relevant_paragraphs.append(paragraph)
        except (wikipedia.exceptions.DisambiguationError, wikipedia.exceptions.PageError):
            continue
    return relevant_paragraphs

def get_bert_embeddings(paragraphs, word):
    all_embeddings = []
    for paragraph in paragraphs:
        inputs = tokenizer(paragraph, return_tensors="pt", truncation=True, padding="max_length", max_length=512)
        with torch.no_grad():
            outputs = model(**inputs)
        last_hidden_state = outputs.last_hidden_state
        word_ids = tokenizer.encode(word, add_special_tokens=False)
        word_idx = (inputs["input_ids"] == word_ids[0]).nonzero(as_tuple=False)
        paragraph_embeddings = [last_hidden_state[0][idx[1]].numpy() for idx in word_idx]
        all_embeddings.extend(paragraph_embeddings)
    return all_embeddings

def horn_parallel_analysis(data, n_permutations=100, random_state=None, label="", plot=True):
    random_state = check_random_state(random_state)
    n_vars, n_samples = data.shape
    real_pca = PCA().fit(data)
    real_eigenvalues = real_pca.explained_variance_

    rand_eigenvalues = []
    for _ in tqdm(range(n_permutations), desc=f"Horn Analysis: {label}"):
        random_data = random_state.normal(size=data.shape)
        random_pca = PCA().fit(random_data)
        rand_eigenvalues.append(random_pca.explained_variance_)

    eigenvalue_thresholds = np.percentile(rand_eigenvalues, 95, axis=0)
    n_components_to_retain = np.sum(real_eigenvalues > eigenvalue_thresholds)

    if plot:
        plt.figure(figsize=(8, 4))
        plt.plot(real_eigenvalues[:50], label="Real Eigenvalues", color="blue")
        plt.plot(eigenvalue_thresholds[:50], label="95th percentile Random Eigenvalues", color="red")
        plt.axvline(n_components_to_retain, color="green", linestyle="--", label=f"{n_components_to_retain} dims")
        plt.title(f"Horn Parallel Analysis - {label.title()}")
        plt.xlabel("Components")
        plt.ylabel("Eigenvalue")
        plt.legend()
        plt.tight_layout()
        plt.show()

    return n_components_to_retain

# ---------------------------
# EJECUCI√ìN POR PA√çS
# ---------------------------

for country in countries:
    print(f"\nüåç Procesando pa√≠s: {country.title()}")
    paragraphs = fetch_paragraphs_with_query(country, max_results=30)

    if not paragraphs:
        print(f"‚ùå No se encontraron p√°rrafos.")
        continue

    embeddings = get_bert_embeddings(paragraphs, country.lower())
    if len(embeddings) < 10:
        print(f"‚ö†Ô∏è Solo {len(embeddings)} embeddings encontrados. Saltando an√°lisis.")
        continue

    X = np.array(embeddings).T
    print(f"‚úîÔ∏è Matriz embeddings: {X.shape[1]} tokens √ó {X.shape[0]} dimensiones")

    try:
        n_components = horn_parallel_analysis(X, random_state=42, label=country)
        print(f"üìê Dimensiones √≥ptimas para {country.title()}: {n_components}")

        # PCA final para subespacio
        pca = PCA(n_components=n_components)
        subspace = pca.fit_transform(X.T).T

        print(f"üìè Subespacio reducido: {subspace.shape}")

    except Exception as e:
        print(f"‚ùå Error procesando {country.title()}: {e}")

"""# PCA y Horn method A"""

import wikipedia

def fetch_paragraphs_with_query(query, max_results=100):
    # Buscar p√°ginas que coincidan con la consulta
    titles = wikipedia.search(query, results=max_results)

    relevant_paragraphs = []
    for title in titles:
        try:
            content = wikipedia.page(title).content
            paragraphs = content.split('\n\n')  # Dividir el contenido en p√°rrafos

            # Filtrar p√°rrafos que contienen la palabra "bat"
            for paragraph in paragraphs:
                if query.lower() in paragraph.lower():
                    relevant_paragraphs.append(paragraph)

        except (wikipedia.exceptions.DisambiguationError, wikipedia.exceptions.PageError):
            continue

    return relevant_paragraphs

china_paragraphs = fetch_paragraphs_with_query("China", max_results=30)
len(china_paragraphs)

from transformers import BertTokenizer, BertModel
import torch

def get_bert_embeddings(paragraphs, word):
    # Cargar el tokenizador y el modelo de BERT
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')

    all_embeddings = []

    # Procesar cada p√°rrafo
    for paragraph in paragraphs:
        # Tokenizar y obtener salida de BERT
        inputs = tokenizer(paragraph, return_tensors="pt", truncation=True, max_length=512, padding='max_length')
        with torch.no_grad():
            outputs = model(**inputs)

        # Tomar embeddings del √∫ltimo layer (cuarentena!!!!)
        last_hidden_state = outputs.last_hidden_state

        # Buscar √≠ndices de la palabra "bat"
        word_ids = tokenizer.encode(word, add_special_tokens=False)
        word_idx = (inputs["input_ids"] == word_ids[0]).nonzero(as_tuple=False)

        # Si la palabra "bat" est√° en el p√°rrafo, recoger todos los embeddings para cada aparici√≥n
        paragraph_embeddings = [last_hidden_state[0][idx[1]].numpy() for idx in word_idx]
        all_embeddings.extend(paragraph_embeddings)

    return all_embeddings

# Obtener embeddings para la palabra "bat" en todos los p√°rrafos relevantes
china_embeddings = get_bert_embeddings(china_paragraphs, "China")
len(china_embeddings)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.utils import check_random_state
from tqdm import tqdm

def horn_parallel_analysis(data, n_permutations=100, random_state=None):
    random_state = check_random_state(random_state)
    n_vars, n_samples = data.shape
    real_pca = PCA().fit(data)
    real_eigenvalues = real_pca.explained_variance_

    rand_eigenvalues = []

    # Using tqdm for progress bar
    for _ in tqdm(range(n_permutations), desc="Running Parallel Analysis"):
        random_data = random_state.normal(size=data.shape)
        random_pca = PCA().fit(random_data)
        rand_eigenvalues.append(random_pca.explained_variance_)

    eigenvalue_thresholds = np.percentile(rand_eigenvalues, 95, axis=0)

    n_components_to_retain = np.sum(real_eigenvalues > eigenvalue_thresholds)

    # Plotting
    plt.plot(real_eigenvalues, label="Real Eigenvalues", color="blue")
    plt.plot(eigenvalue_thresholds, label="95th percentile Random Eigenvalues", color="red")
    plt.xlabel("Components")
    plt.ylabel("Eigenvalue")
    plt.legend()
    plt.title(f"Parallel Analysis: {n_components_to_retain} components to retain")
    plt.show()

    return n_components_to_retain

# Generate some example data
X = np.array(china_embeddings).T

# Use function
n_components = horn_parallel_analysis(X)
print(f"Number of components to retain: {n_components}")

"""# Method B

## Intento 1
"""

import wikipedia
from transformers import BertTokenizer, BertModel
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.utils import check_random_state
from tqdm import tqdm

# Configurar Wikipedia en ingl√©s
wikipedia.set_lang("en")

# Lista de pa√≠ses
countries = [
    "ghana", "belize", "slovenia", "slovakia", "zambia", "zimbabwe", "laos", "cambodia", "yemen",
    "turkmenistan", "uzbekistan", "luxembourg", "france", "bolivia", "united states", "china", "mexico", "russia",
    "belgium", "andorra", "spain", "belarus", "faroe islands", "argentina", "paraguay", "brazil",
    "germany", "canada", "japan", "italy", "morocco", "india", "united kingdom", "singapore", "sweden", "norway",
    "portugal", "ecuador", "peru", "burkina faso", "french guiana", "mongolia", "bulgaria", "oman"
]

# Funci√≥n para obtener p√°rrafos relevantes desde Wikipedia
def fetch_paragraphs_with_query(query, max_results=30):
    titles = wikipedia.search(query, results=max_results)
    relevant_paragraphs = []
    for title in titles:
        try:
            content = wikipedia.page(title).content
            paragraphs = content.split('\n\n')
            for paragraph in paragraphs:
                if query.lower() in paragraph.lower():
                    relevant_paragraphs.append(paragraph)
        except (wikipedia.exceptions.DisambiguationError, wikipedia.exceptions.PageError):
            continue
    return relevant_paragraphs

# Funci√≥n para obtener embeddings de BERT de un t√©rmino en contexto
def get_bert_embeddings(paragraphs, word):
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')
    all_embeddings = []

    for paragraph in paragraphs:
        inputs = tokenizer(paragraph, return_tensors="pt", truncation=True, max_length=512, padding='max_length')
        with torch.no_grad():
            outputs = model(**inputs)

        last_hidden_state = outputs.last_hidden_state
        word_ids = tokenizer.encode(word, add_special_tokens=False)
        word_idx = (inputs["input_ids"] == word_ids[0]).nonzero(as_tuple=False)
        paragraph_embeddings = [last_hidden_state[0][idx[1]].numpy() for idx in word_idx]
        all_embeddings.extend(paragraph_embeddings)

    return all_embeddings

# An√°lisis Paralelo de Horn
def horn_parallel_analysis(data, n_permutations=100, random_state=None, plot=True, label=""):
    random_state = check_random_state(random_state)
    n_vars, n_samples = data.shape
    real_pca = PCA().fit(data)
    real_eigenvalues = real_pca.explained_variance_

    rand_eigenvalues = []
    for _ in tqdm(range(n_permutations), desc="Running Parallel Analysis"):
        random_data = random_state.normal(size=data.shape)
        random_pca = PCA().fit(random_data)
        rand_eigenvalues.append(random_pca.explained_variance_)

    eigenvalue_thresholds = np.percentile(rand_eigenvalues, 95, axis=0)
    n_components_to_retain = np.sum(real_eigenvalues > eigenvalue_thresholds)

    if plot:
        plt.figure(figsize=(8, 4))
        plt.plot(real_eigenvalues, label="Real Eigenvalues", color="blue")
        plt.plot(eigenvalue_thresholds, label="95th percentile Random Eigenvalues", color="red")
        plt.axvline(n_components_to_retain, color="green", linestyle="--", label=f"{n_components_to_retain} dims")
        plt.xlabel("Components")
        plt.ylabel("Eigenvalue")
        plt.title(f"Horn Parallel Analysis - {label}")
        plt.legend()
        plt.tight_layout()
        plt.show()

    return n_components_to_retain

# Loop sobre pa√≠ses
for country in countries:
    print(f"\nüåç Procesando pa√≠s: {country.title()}")
    paragraphs = fetch_paragraphs_with_query(country, max_results=30)
    if not paragraphs:
        print(f"‚ùå No se encontraron p√°rrafos para {country.title()}.")
        continue

    embeddings = get_bert_embeddings(paragraphs, country.lower())
    if len(embeddings) < 10:
        print(f"‚ö†Ô∏è Muy pocos embeddings ({len(embeddings)}). Saltando an√°lisis.")
        continue

    X = np.array(embeddings).T
    print(f"‚úîÔ∏è Total embeddings: {X.shape[1]} (cada uno con {X.shape[0]} dimensiones)")

    # An√°lisis paralelo
    try:
        n_components = horn_parallel_analysis(X, n_permutations=100, random_state=42, plot=True, label=country.title())
        print(f"üìê Dimensiones √≥ptimas para {country.title()}: {n_components}")

        # PCA final para obtener subespacio reducido
        pca = PCA(n_components=n_components)
        subspace = pca.fit_transform(X).T  # Transpuesta para conservar forma original

        print(f"üìè Subespacio reducido: {subspace.shape}")
    except Exception as e:
        print(f"‚ùå Error procesando {country.title()}: {e}")

"""## Intento 2"""

import wikipedia
from transformers import BertTokenizer, BertModel
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.utils import check_random_state
from tqdm import tqdm

# Configurar Wikipedia en ingl√©s
wikipedia.set_lang("en")

# Lista de pa√≠ses
countries = [ "china", "mexico", "russia",
    "belgium", "andorra", "spain", "belarus", "faroe islands", "argentina", "paraguay", "brazil",
    "germany", "canada", "japan", "italy", "morocco", "india", "united kingdom", "singapore", "sweden", "norway",
    "portugal", "ecuador", "peru", "burkina faso", "french guiana", "mongolia", "bulgaria", "oman"
]

# Funci√≥n para obtener p√°rrafos relevantes desde Wikipedia
def fetch_paragraphs_with_query(query, max_results=30):
    titles = wikipedia.search(query, results=max_results)
    relevant_paragraphs = []
    for title in titles:
        try:
            content = wikipedia.page(title).content
            paragraphs = content.split('\n\n')
            for paragraph in paragraphs:
                if query.lower() in paragraph.lower():
                    relevant_paragraphs.append(paragraph)
        except (wikipedia.exceptions.DisambiguationError, wikipedia.exceptions.PageError):
            continue
    return relevant_paragraphs

# Funci√≥n para obtener embeddings de BERT de un t√©rmino en contexto
def get_bert_embeddings(paragraphs, word):
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')
    all_embeddings = []

    for paragraph in paragraphs:
        inputs = tokenizer(paragraph, return_tensors="pt", truncation=True, max_length=512, padding='max_length')
        with torch.no_grad():
            outputs = model(**inputs)

        last_hidden_state = outputs.last_hidden_state
        word_ids = tokenizer.encode(word, add_special_tokens=False)
        word_idx = (inputs["input_ids"] == word_ids[0]).nonzero(as_tuple=False)
        paragraph_embeddings = [last_hidden_state[0][idx[1]].numpy() for idx in word_idx]
        all_embeddings.extend(paragraph_embeddings)

    return all_embeddings

# An√°lisis Paralelo de Horn
def horn_parallel_analysis(data, n_permutations=100, random_state=None, plot=True, label=""):
    random_state = check_random_state(random_state)
    n_vars, n_samples = data.shape
    real_pca = PCA().fit(data)
    real_eigenvalues = real_pca.explained_variance_

    rand_eigenvalues = []
    for _ in tqdm(range(n_permutations), desc="Running Parallel Analysis"):
        random_data = random_state.normal(size=data.shape)
        random_pca = PCA().fit(random_data)
        rand_eigenvalues.append(random_pca.explained_variance_)

    eigenvalue_thresholds = np.percentile(rand_eigenvalues, 95, axis=0)
    n_components_to_retain = np.sum(real_eigenvalues > eigenvalue_thresholds)

    if plot:
        plt.figure(figsize=(8, 4))
        plt.plot(real_eigenvalues, label="Real Eigenvalues", color="blue")
        plt.plot(eigenvalue_thresholds, label="95th percentile Random Eigenvalues", color="red")
        plt.axvline(n_components_to_retain, color="green", linestyle="--", label=f"{n_components_to_retain} dims")
        plt.xlabel("Components")
        plt.ylabel("Eigenvalue")
        plt.title(f"Horn Parallel Analysis - {label}")
        plt.legend()
        plt.tight_layout()
        plt.show()

    return n_components_to_retain

# Loop sobre pa√≠ses
for country in countries:
    print(f"\nüåç Procesando pa√≠s: {country.title()}")
    paragraphs = fetch_paragraphs_with_query(country, max_results=30)
    if not paragraphs:
        print(f"‚ùå No se encontraron p√°rrafos para {country.title()}.")
        continue

    embeddings = get_bert_embeddings(paragraphs, country.lower())
    if len(embeddings) < 10:
        print(f"‚ö†Ô∏è Muy pocos embeddings ({len(embeddings)}). Saltando an√°lisis.")
        continue

    X = np.array(embeddings).T
    print(f"‚úîÔ∏è Total embeddings: {X.shape[1]} (cada uno con {X.shape[0]} dimensiones)")

    # An√°lisis paralelo
    try:
        n_components = horn_parallel_analysis(X, n_permutations=100, random_state=42, plot=True, label=country.title())
        print(f"üìê Dimensiones √≥ptimas para {country.title()}: {n_components}")

        # PCA final para obtener subespacio reducido
        pca = PCA(n_components=n_components)
        subspace = pca.fit_transform(X.T).T  # Transpuesta para conservar forma original

        print(f"üìè Subespacio reducido: {subspace.shape}")
    except Exception as e:
        print(f"‚ùå Error procesando {country.title()}: {e}")

"""## Intento 3"""

import wikipedia
from transformers import BertTokenizer, BertModel
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.utils import check_random_state
from tqdm import tqdm

# Configurar Wikipedia en ingl√©s
wikipedia.set_lang("en")

# Lista de pa√≠ses
countries = [ "argentina", "paraguay", "brazil",
    "germany", "canada", "japan", "italy", "morocco", "india", "united kingdom", "singapore", "sweden", "norway",
    "portugal", "ecuador", "peru", "burkina faso", "french guiana", "mongolia", "bulgaria", "oman"
]

# Funci√≥n para obtener p√°rrafos relevantes desde Wikipedia
def fetch_paragraphs_with_query(query, max_results=30):
    titles = wikipedia.search(query, results=max_results)
    relevant_paragraphs = []
    for title in titles:
        try:
            content = wikipedia.page(title).content
            paragraphs = content.split('\n\n')
            for paragraph in paragraphs:
                if query.lower() in paragraph.lower():
                    relevant_paragraphs.append(paragraph)
        except (wikipedia.exceptions.DisambiguationError, wikipedia.exceptions.PageError):
            continue
    return relevant_paragraphs

# Funci√≥n para obtener embeddings de BERT de un t√©rmino en contexto
def get_bert_embeddings(paragraphs, word):
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')
    all_embeddings = []

    for paragraph in paragraphs:
        inputs = tokenizer(paragraph, return_tensors="pt", truncation=True, max_length=512, padding='max_length')
        with torch.no_grad():
            outputs = model(**inputs)

        last_hidden_state = outputs.last_hidden_state
        word_ids = tokenizer.encode(word, add_special_tokens=False)
        word_idx = (inputs["input_ids"] == word_ids[0]).nonzero(as_tuple=False)
        paragraph_embeddings = [last_hidden_state[0][idx[1]].numpy() for idx in word_idx]
        all_embeddings.extend(paragraph_embeddings)

    return all_embeddings

# An√°lisis Paralelo de Horn
def horn_parallel_analysis(data, n_permutations=100, random_state=None, plot=True, label=""):
    random_state = check_random_state(random_state)
    n_vars, n_samples = data.shape
    real_pca = PCA().fit(data)
    real_eigenvalues = real_pca.explained_variance_

    rand_eigenvalues = []
    for _ in tqdm(range(n_permutations), desc="Running Parallel Analysis"):
        random_data = random_state.normal(size=data.shape)
        random_pca = PCA().fit(random_data)
        rand_eigenvalues.append(random_pca.explained_variance_)

    eigenvalue_thresholds = np.percentile(rand_eigenvalues, 95, axis=0)
    n_components_to_retain = np.sum(real_eigenvalues > eigenvalue_thresholds)

    if plot:
        plt.figure(figsize=(8, 4))
        plt.plot(real_eigenvalues, label="Real Eigenvalues", color="blue")
        plt.plot(eigenvalue_thresholds, label="95th percentile Random Eigenvalues", color="red")
        plt.axvline(n_components_to_retain, color="green", linestyle="--", label=f"{n_components_to_retain} dims")
        plt.xlabel("Components")
        plt.ylabel("Eigenvalue")
        plt.title(f"Horn Parallel Analysis - {label}")
        plt.legend()
        plt.tight_layout()
        plt.show()

    return n_components_to_retain

# Loop sobre pa√≠ses
for country in countries:
    print(f"\nüåç Procesando pa√≠s: {country.title()}")
    paragraphs = fetch_paragraphs_with_query(country, max_results=30)
    if not paragraphs:
        print(f"‚ùå No se encontraron p√°rrafos para {country.title()}.")
        continue

    embeddings = get_bert_embeddings(paragraphs, country.lower())
    if len(embeddings) < 10:
        print(f"‚ö†Ô∏è Muy pocos embeddings ({len(embeddings)}). Saltando an√°lisis.")
        continue

    X = np.array(embeddings).T
    print(f"‚úîÔ∏è Total embeddings: {X.shape[1]} (cada uno con {X.shape[0]} dimensiones)")

    # An√°lisis paralelo
    try:
        n_components = horn_parallel_analysis(X, n_permutations=100, random_state=42, plot=True, label=country.title())
        print(f"üìê Dimensiones √≥ptimas para {country.title()}: {n_components}")

        # PCA final para obtener subespacio reducido
        pca = PCA(n_components=n_components)
        subspace = pca.fit_transform(X.T).T  # Transpuesta para conservar forma original

        print(f"üìè Subespacio reducido: {subspace.shape}")
    except Exception as e:
        print(f"‚ùå Error procesando {country.title()}: {e}")

"""## Intento 4"""

import wikipedia
from transformers import BertTokenizer, BertModel
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.utils import check_random_state
from tqdm import tqdm

# Configurar Wikipedia en ingl√©s
wikipedia.set_lang("en")

# Lista de pa√≠ses
countries = [  "morocco", "india", "united kingdom", "singapore", "sweden", "norway",
    "portugal", "ecuador", "peru", "burkina faso", "french guiana", "mongolia", "bulgaria", "oman"
]

# Funci√≥n para obtener p√°rrafos relevantes desde Wikipedia
def fetch_paragraphs_with_query(query, max_results=30):
    titles = wikipedia.search(query, results=max_results)
    relevant_paragraphs = []
    for title in titles:
        try:
            content = wikipedia.page(title).content
            paragraphs = content.split('\n\n')
            for paragraph in paragraphs:
                if query.lower() in paragraph.lower():
                    relevant_paragraphs.append(paragraph)
        except (wikipedia.exceptions.DisambiguationError, wikipedia.exceptions.PageError):
            continue
    return relevant_paragraphs

# Funci√≥n para obtener embeddings de BERT de un t√©rmino en contexto
def get_bert_embeddings(paragraphs, word):
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')
    all_embeddings = []

    for paragraph in paragraphs:
        inputs = tokenizer(paragraph, return_tensors="pt", truncation=True, max_length=512, padding='max_length')
        with torch.no_grad():
            outputs = model(**inputs)

        last_hidden_state = outputs.last_hidden_state
        word_ids = tokenizer.encode(word, add_special_tokens=False)
        word_idx = (inputs["input_ids"] == word_ids[0]).nonzero(as_tuple=False)
        paragraph_embeddings = [last_hidden_state[0][idx[1]].numpy() for idx in word_idx]
        all_embeddings.extend(paragraph_embeddings)

    return all_embeddings

# An√°lisis Paralelo de Horn
def horn_parallel_analysis(data, n_permutations=100, random_state=None, plot=True, label=""):
    random_state = check_random_state(random_state)
    n_vars, n_samples = data.shape
    real_pca = PCA().fit(data)
    real_eigenvalues = real_pca.explained_variance_

    rand_eigenvalues = []
    for _ in tqdm(range(n_permutations), desc="Running Parallel Analysis"):
        random_data = random_state.normal(size=data.shape)
        random_pca = PCA().fit(random_data)
        rand_eigenvalues.append(random_pca.explained_variance_)

    eigenvalue_thresholds = np.percentile(rand_eigenvalues, 95, axis=0)
    n_components_to_retain = np.sum(real_eigenvalues > eigenvalue_thresholds)

    if plot:
        plt.figure(figsize=(8, 4))
        plt.plot(real_eigenvalues, label="Real Eigenvalues", color="blue")
        plt.plot(eigenvalue_thresholds, label="95th percentile Random Eigenvalues", color="red")
        plt.axvline(n_components_to_retain, color="green", linestyle="--", label=f"{n_components_to_retain} dims")
        plt.xlabel("Components")
        plt.ylabel("Eigenvalue")
        plt.title(f"Horn Parallel Analysis - {label}")
        plt.legend()
        plt.tight_layout()
        plt.show()

    return n_components_to_retain

# Loop sobre pa√≠ses
for country in countries:
    print(f"\nüåç Procesando pa√≠s: {country.title()}")
    paragraphs = fetch_paragraphs_with_query(country, max_results=30)
    if not paragraphs:
        print(f"‚ùå No se encontraron p√°rrafos para {country.title()}.")
        continue

    embeddings = get_bert_embeddings(paragraphs, country.lower())
    if len(embeddings) < 10:
        print(f"‚ö†Ô∏è Muy pocos embeddings ({len(embeddings)}). Saltando an√°lisis.")
        continue

    X = np.array(embeddings).T
    print(f"‚úîÔ∏è Total embeddings: {X.shape[1]} (cada uno con {X.shape[0]} dimensiones)")

    # An√°lisis paralelo
    try:
        n_components = horn_parallel_analysis(X, n_permutations=100, random_state=42, plot=True, label=country.title())
        print(f"üìê Dimensiones √≥ptimas para {country.title()}: {n_components}")

        # PCA final para obtener subespacio reducido
        pca = PCA(n_components=n_components)
        subspace = pca.fit_transform(X.T).T  # Transpuesta para conservar forma original

        print(f"üìè Subespacio reducido: {subspace.shape}")
    except Exception as e:
        print(f"‚ùå Error procesando {country.title()}: {e}")